{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单期神经网络训练"
   ]
  },
  {
   "source": [
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import trange\n",
    "\n",
    "import dataset\n",
    "from Nets import RED_CNN\n",
    "from Loss_Func import SSIM_Loss, MSE_Loss\n",
    "from Model_Test import ModelTest"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDCT_path = r'E:\\NBIA\\Sampling\\Dataset\\LDCT5set'\n",
    "NDCT_path = r'E:\\NBIA\\Sampling\\Dataset\\NDCT5set'\n",
    "\n",
    "my_totensor = dataset.My_ToTensor()\n",
    "my_normalize = dataset.My_Normalize(0.1225, 0.1188)\n",
    "transform = dataset.My_Compose([my_totensor])\n",
    "normalize = dataset.My_Compose([my_normalize])\n",
    "\n",
    "train_set = dataset.Mydataset(LDCT_root = LDCT_path, NDCT_root = NDCT_path, train = True, \n",
    "                      transform = transform, \n",
    "                      normalize = normalize)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size = 16, \n",
    "                                           num_workers = 4,\n",
    "                                           shuffle = True,)\n",
    "print(train_set.len)"
   ]
  },
  {
   "source": [
    "## 2、构建神经网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RED_CNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction = 'mean') #定义损失函数：均方误差\n",
    "# criterion = MSE_Loss()\n",
    "# criterion = SSIM_Loss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001) #定义优化方法，Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、模型训练\n",
    "### a.参数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------训练设置------#\n",
    "model_name = 'RED_CNN_MSE'\n",
    "train_epoch_num = 10\n",
    "First_train = False\n",
    "train_adjust = False\n",
    "\n",
    "#----------------------------分割线----------------------------#\n",
    "writer = SummaryWriter('./{}/{}'.format(model_name, 'tensorboard'))\n",
    "loss_path = r'./{0}/{0}_Loss.npy'.format(model_name)\n",
    "epochs_loss_path = r'./{0}/{0}_Epochs_Loss.npy'.format(model_name)\n",
    "if not os.path.isdir('./{}'.format(model_name)):\n",
    "    os.mkdir('./{}'.format(model_name))\n",
    "if First_train:\n",
    "    epoch_now = 0\n",
    "    all_epoch_loss = []\n",
    "    epochs_loss = []\n",
    "else:\n",
    "    model_num = int(input('The num of the last Epoch: '))\n",
    "    model_path = r'./{0}/checkpoint/{0}_epoch_{1}.ckpt'.format(model_name, model_num)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.cuda()\n",
    "    epoch_now = checkpoint['epoch']\n",
    "    all_epoch_loss = np.load(loss_path)\n",
    "    all_epoch_loss = list(all_epoch_loss)\n",
    "    epochs_loss = np.load(epochs_loss_path)\n",
    "    epochs_loss = list(epochs_loss)\n",
    "    if train_adjust:\n",
    "        file_path = r'./{0}/{0}-{1}.json'.format(model_name,model_num)\n",
    "        with open(file_path, 'r') as f:\n",
    "            info_dict = json.load(f)\n",
    "        all_epoch_loss  = info_dict['all_epoch_loss']\n",
    "        epochs_loss  = info_dict['epochs_loss']\n",
    "    print('Epoch:', epoch_now)\n",
    "    print(len(all_epoch_loss))\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.75, last_epoch = epoch_now-1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.cuda()\n",
    "net.train()\n",
    "\n",
    "for epoch in range(train_epoch_num):\n",
    "    dataiter = iter(train_loader)\n",
    "    loss_list = []\n",
    "    \n",
    "    for batch_idx in trange(len(train_loader)):\n",
    "        #初始化\n",
    "        data = dataiter.next()\n",
    "        LDCT_img, NDCT_img = data\n",
    "        LDCT_img = LDCT_img.cuda()\n",
    "        NDCT_img = NDCT_img.cpu()\n",
    "\n",
    "        # 将梯度设置为0\n",
    "        optimizer.zero_grad()              \n",
    "\n",
    "        predicted = net(LDCT_img)\n",
    "        predicted = predicted.cpu()\n",
    "        #预测结果predicted_res和residuals通过之前定义的MSE计算损失\n",
    "        loss = criterion(predicted, NDCT_img)\n",
    "\n",
    "        # 误差反向传播MSE\n",
    "        loss.backward()\n",
    "\n",
    "        # Adam优化权重\n",
    "        optimizer.step()                   \n",
    "\n",
    "        # 保存Loss\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    mean_loss = sum(loss_list)/len(loss_list)\n",
    "\n",
    "    #保存Loss数据\n",
    "    epochs_loss.append(mean_loss)\n",
    "    save_epochs_loss = np.array(epochs_loss)\n",
    "    np.save(epochs_loss_path, save_epochs_loss)\n",
    "    all_epoch_loss += loss_list\n",
    "    save_loss = np.array(all_epoch_loss)\n",
    "    np.save(loss_path, save_loss)\n",
    "\n",
    "    print('[epoch %d]: %.10f' % (epoch+epoch_now+1, mean_loss))\n",
    "    plt.figure(figsize=(15,3))\n",
    "    plt.subplot(131), plt.title('Epoch{}'.format(epoch+epoch_now+1)), plt.plot(loss_list, color='teal')\n",
    "    plt.subplot(132), plt.title('All_Loss'), plt.plot(all_epoch_loss, color='teal')\n",
    "    plt.subplot(133), plt.title('Epochs_Loss'), plt.plot(epochs_loss, color='teal')\n",
    "    plt.tight_layout(pad=0, w_pad=3, h_pad=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    print('Saving epoch %d model ...' % (epoch+epoch_now+1))\n",
    "\n",
    "    state = {'net': net.state_dict(), 'optimizer': optimizer.state_dict(),'epoch': epoch+epoch_now+1}\n",
    "    if not os.path.isdir('./{}/checkpoint'.format(model_name)):\n",
    "        os.mkdir('./{}/checkpoint'.format(model_name))\n",
    "    torch.save(state, './{0}/checkpoint/{0}_epoch_{1}.ckpt'.format(model_name, epoch+epoch_now+1))\n",
    "    #调整Lr\n",
    "    scheduler.step()\n",
    "all_loss = np.load(loss_path)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121), plt.title('All_Loss'), plt.plot(all_epoch_loss, color='teal')\n",
    "plt.subplot(122), plt.title('Epochs_Loss'), plt.plot(epochs_loss, color='teal')\n",
    "plt.tight_layout(pad=1, w_pad=3, h_pad=0.5)\n",
    "plt.savefig('./{}/Epoch-{}.jpg'.format(model_name, epoch+epoch_now+1), dpi = 500, bbox_inches = 'tight', pad_inches = 0.25)\n",
    "plt.show()\n",
    "\n",
    "torch.save(net, './{0}/checkpoint/{0}_epoch_{1}.pt'.format(model_name, epoch+epoch_now+1))\n",
    "print('Finished Training...')\n",
    "for idx in range(len(epochs_loss)):\n",
    "    print('Epoch{:3}: '.format(idx+1), epochs_loss[idx])"
   ]
  },
  {
   "source": [
    "### c.保存训练数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "res_dict['model_name'] = model_name\n",
    "res_dict['loss_func'] = 'MSE'\n",
    "res_dict['loss_decay'] = 0.75\n",
    "res_dict['batch_size'] = 16\n",
    "res_dict['model_config'] = 'RED_CNN_SSIM'\n",
    "# res_dict['model_config'] = 'DenseNet(growth_rate=16, block_config=(4, 8, 4), num_init_features=64, bn_size=4)'\n",
    "res_dict['net'] = str(net)\n",
    "res_dict['epochs'] = '{}'.format(epoch+epoch_now+1)\n",
    "res_dict['epochs_loss'] = epochs_loss\n",
    "res_dict['all_epoch_loss'] = all_epoch_loss\n",
    "file_path = r'./{0}/{0}-{1}.json'.format(model_name,epoch+epoch_now+1)\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(res_dict, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、模型测试\n",
    "### a.测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDCT_path = r'E:\\NBIA\\LDCT-and-Projection-data\\L123\\08-23-2018-75696\\1.000000-Low Dose Images-07574'\n",
    "NDCT_path = r'E:\\NBIA\\LDCT-and-Projection-data\\L123\\08-23-2018-75696\\1.000000-Full dose images-67226'\n",
    "test_epoch = 30\n",
    "model_path = r'./{0}/checkpoint/{0}_epoch_{1}.pt'.format(model_name, test_epoch)\n",
    "save_path = r'./{}/Test_Result'.format(model_name)\n",
    "test = ModelTest(model_name, test_epoch, model_path, LDCT_path, NDCT_path, save_path, stage_num=1)\n",
    "test.run()"
   ]
  },
  {
   "source": [
    "b.单图测试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_epoch = 20\n",
    "LD_path = r'E:\\NBIA\\LDCT-and-Projection-data\\L123\\08-23-2018-75696\\1.000000-Low Dose Images-07574\\1-040.dcm'\n",
    "ND_path = r'E:\\NBIA\\LDCT-and-Projection-data\\L123\\08-23-2018-75696\\1.000000-Full dose images-67226\\1-040.dcm'\n",
    "\n",
    "net_test = torch.load('./{0}/checkpoint/{0}_epoch_{1}.pt'.format(model_name, test_epoch))\n",
    "\n",
    "net_test.eval()\n",
    "net_test.cuda()\n",
    "\n",
    "import pydicom.filereader as dcmreader\n",
    "import pydicom.dataset as dcmdt\n",
    "\n",
    "def Window(WW, WL):\n",
    "    win_dict = {'vmin':WL-WW/2, 'vmax':WL+WW/2}\n",
    "    return win_dict\n",
    "def array2tensor(image_array):\n",
    "    image_array = image_array[:, :, None]\n",
    "    image_array = torch.from_numpy(image_array.transpose((2, 0, 1))).contiguous()\n",
    "    image_array = torch.stack([image_array])\n",
    "    image_tensor = image_array/1.0\n",
    "    return image_tensor\n",
    "\n",
    "LD_ds = dcmreader.dcmread(LD_path)\n",
    "ND_ds = dcmreader.dcmread(ND_path)\n",
    "LD_img = LD_ds.pixel_array.astype(np.int16)\n",
    "ND_img = ND_ds.pixel_array.astype(np.int16)\n",
    "\n",
    "my_totensor = dataset.My_ToTensor()\n",
    "my_normalize = dataset.My_Normalize(0.131, 0.121)\n",
    "loader = dataset.My_Compose([my_totensor, my_normalize])\n",
    "\n",
    "Res_img = LD_img - ND_img\n",
    "print(ND_img.shape)\n",
    "plt.subplot(131), plt.title('ND'), plt.imshow(ND_img-1024, cmap = plt.cm.Greys_r, **Window(300, 40)), plt.axis('off')\n",
    "plt.subplot(132), plt.title('LD'), plt.imshow(LD_img-1024, cmap = plt.cm.Greys_r, **Window(300, 40)), plt.axis('off')\n",
    "plt.subplot(133), plt.title('Res'),  plt.imshow(Res_img, cmap = plt.cm.Greys_r), plt.axis('off')\n",
    "plt.show()\n",
    "LD_img = loader(LD_img)\n",
    "ND_img = loader(ND_img)\n",
    "LD_img = torch.stack([LD_img])\n",
    "ND_img = ND_img.squeeze()\n",
    "ND_img = ND_img.detach().numpy()\n",
    "with torch.no_grad():\n",
    "    LD_img = LD_img.cuda()\n",
    "    output = net_test(LD_img)\n",
    "    pre = output.cpu()\n",
    "    LD_img = LD_img.cpu()\n",
    "    LD_img = LD_img.squeeze()\n",
    "    LD_img = LD_img.detach().numpy()\n",
    "    pre = pre.squeeze()\n",
    "    pre = pre.detach().numpy()\n",
    "\n",
    "plt.subplot(231), plt.axis('off'), plt.title('NDCT'),             plt.imshow(((ND_img*0.1188)+0.1225)*4096 - 1024,\n",
    "                                                                             cmap = plt.cm.Greys_r, **Window(300, 40))\n",
    "plt.subplot(232), plt.axis('off'), plt.title('LDCT'),             plt.imshow(((LD_img*0.1188)+0.1225)*4096 - 1024,\n",
    "                                                                             cmap = plt.cm.Greys_r, **Window(300, 40))\n",
    "plt.subplot(233), plt.axis('off'), plt.title('Predicted'),        plt.imshow(((pre*0.1188)+0.1225)*4096 - 1024,\n",
    "                                                                             cmap = plt.cm.Greys_r, **Window(300, 40))\n",
    "plt.subplot(234), plt.axis('off'), plt.title('LDCT - NDCT'),      plt.imshow(LD_img - ND_img, cmap = plt.cm.Greys_r)\n",
    "plt.subplot(235), plt.axis('off'), plt.title('LDCT - Predicted'), plt.imshow(LD_img - pre, cmap = plt.cm.Greys_r)\n",
    "plt.subplot(236), plt.axis('off'), plt.title('Predicted - NDCT'), plt.imshow(pre - ND_img,\n",
    "                                                                             cmap = plt.cm.Greys_r)\n",
    "(0.1225, 0.1188)\n",
    "plt.imsave('./{}/{}-1 ND.jpg'.format(model_name, test_epoch), ND_img, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-2 LD.jpg'.format(model_name, test_epoch), LD_img, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-3 pre.jpg'.format(model_name, test_epoch), pre, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-4 pre_Noise.jpg'.format(model_name, test_epoch), LD_img - pre, cmap = plt.cm.Greys_r)\n",
    "plt.imsave('./{}/{}-5 rea_Noise.jpg'.format(model_name, test_epoch), LD_img - ND_img, cmap = plt.cm.Greys_r)\n",
    "plt.savefig('./{}/Epoch-{}.jpg'.format(model_name, test_epoch), dpi = 500, bbox_inches = 'tight', pad_inches = 0.25)\n",
    "\n",
    "def mseloss(x, y):\n",
    "    return np.sum((x-y)**2)/(512*512)\n",
    "\n",
    "LDloss = mseloss(LD_img, ND_img)\n",
    "preloss = mseloss(pre, ND_img)\n",
    "print('LD:  ', LDloss,  10*np.log10(2*2/LDloss))\n",
    "print('pre: ', preloss, 10*np.log10(2*2/preloss))\n",
    "\n",
    "pn = (((pre*0.1188)+0.1225)*4096)\n",
    "pn = np.clip(pn, 0, 4096)\n",
    "pn = pn.astype(np.uint16)\n",
    "\n",
    "ND_ds.SeriesDescription = 'NDCT'\n",
    "LD_ds.SeriesDescription = 'LDCT'\n",
    "ND_ds.save_as(\"./{}/NDCT.dcm\".format(model_name))\n",
    "LD_ds.save_as(\"./{}/LDCT.dcm\".format(model_name))\n",
    "\n",
    "ND_ds.SeriesDescription = 'Predicted'\n",
    "ND_ds.PixelData = pn.tobytes()\n",
    "ND_ds.Rows, ND_ds.Columns = pn.shape\n",
    "ND_ds.save_as(\"./{}/Predicted.dcm\".format(model_name))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/Arctic_Beacon/article/details/85294426?depth_1-\n",
    "https://blog.csdn.net/jzwong/article/details/104337960?depth_1-\n",
    "https://blog.csdn.net/peacefairy/article/details/108020179\n",
    "https://discuss.pytorch.org/t/torchvision-transforms-functional-normalize-tensor-mean-std/35098"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0f7a73bb8ca7f678f26aafc5e0ccf3837630645dd5ed8595cc84348c81285d859",
   "display_name": "Python 3.8.5 64-bit ('LDCT': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}