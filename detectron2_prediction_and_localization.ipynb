{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "detectron2-prediction-and-localization",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paul-mwaura/NeuralSight/blob/main/detectron2_prediction_and_localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VinBigData detectron2 prediction\n",
        "\n",
        "\n",
        "**Following from the training kernel [VinBigData detectron2 train](https://www.kaggle.com/corochann/vinbigdata-detectron2-train), I will try prediction with the `detectron2` trained model**\n",
        "\n",
        "`detectron2` is one of the famous pytorch object detection library, I will introduce how to use this library to predict bounding boxes with the trained model.\n",
        "\n",
        " - https://github.com/facebookresearch/detectron2\n",
        "\n",
        "> Detectron2 is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. It is a ground-up rewrite of the previous version, Detectron, and it originates from maskrcnn-benchmark.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKoG_itG-ci-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "** [Prediction method implementations](#pred_method)** <br/>\n",
        "** [Prediction scripts](#pred_scripts)** <br/>\n",
        "** [Apply 2 class filter](#2class)** <br/>\n",
        "** [Other kernels](#ref)** <br/>\n",
        "\n",
        "Since first setup part is same with the training kernel, I skipped listing on ToC."
      ],
      "metadata": {
        "id": "M3W0txt7-cjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation\n",
        "\n",
        "Preprocessing x-ray image format (dicom) into normal png image format is already done.\n",
        "\n",
        "Here I will just use the dataset [VinBigData Chest X-ray Resized PNG (256x256)](https://www.kaggle.com/xhlulu/vinbigdata-chest-xray-resized-png-256x256) to skip the preprocessing and focus on modeling part. Please upvote the dataset as well!"
      ],
      "metadata": {
        "id": "wxFz894v-cjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import sys\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "# --- plotly ---\n",
        "from plotly import tools, subplots\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_dark\"\n",
        "\n",
        "# --- models ---\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "\n",
        "# --- setup ---\n",
        "pd.set_option('max_columns', 50)\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "y1rGVXnR-cjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "\n",
        "detectron2 is not pre-installed in this kaggle docker, so let's install it. \n",
        "We can follow [installation instruction](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md), we need to know CUDA and pytorch version to install correct `detectron2`."
      ],
      "metadata": {
        "id": "DM8IJLDF-cjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "WXbXVZrS-cjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "r7xO9uC1-cjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "aSTLQaHb-cjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems CUDA=10.2 and torch==1.7.0 is used in this kaggle docker image.\n",
        "\n",
        "See [installation](https://detectron2.readthedocs.io/tutorials/install.html) for details."
      ],
      "metadata": {
        "id": "SnuWJsxi-cjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install detectron2 -f \\\n",
        "  https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "4GF8w1KS-cjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"pred_method\"></a>\n",
        "# Prediction method implementations\n",
        "\n",
        "Basically we don't need to implement neural network part, `detectron2` already implements famous architectures and provides its pre-trained weights. We can finetune these pre-trained architectures.\n",
        "\n",
        "These models are summarized in [MODEL_ZOO.md](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md).\n",
        "\n",
        "In this competition, we need object detection model, I will choose [R50-FPN](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml) for this kernel."
      ],
      "metadata": {
        "id": "RAnkL4GY-cjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n",
        "\n",
        "`detectron2` provides high-level API for training custom dataset.\n",
        "\n",
        "To define custom dataset, we need to create **list of dict** where each dict contains following:\n",
        "\n",
        " - file_name: file name of the image.\n",
        " - image_id: id of the image, index is used here.\n",
        " - height: height of the image.\n",
        " - width: width of the image.\n",
        " - annotation: This is the ground truth annotation data for object detection, which contains following\n",
        "     - bbox: bounding box pixel location with shape (n_boxes, 4)\n",
        "     - bbox_mode: `BoxMode.XYXY_ABS` is used here, meaning that absolute value of (xmin, ymin, xmax, ymax) annotation is used in the `bbox`.\n",
        "     - category_id: class label id for each bounding box, with shape (n_boxes,)\n",
        "\n",
        "`get_vinbigdata_dicts` is for train dataset preparation and `get_vinbigdata_dicts_test` is for test dataset preparation."
      ],
      "metadata": {
        "id": "4d7NPfgw-cjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../content/vinbigdata-chest-xray-abnormalities-detection/train.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RpvZqRXu-cjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "sWhAbfv0-cjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.class_name.unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "EKdDRP5o-cjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from detectron2.structures import BoxMode\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_vinbigdata_dicts(\n",
        "                            imgdir: Path,\n",
        "                            train_df: pd.DataFrame,\n",
        "                            train_data_type: str = \"original\",\n",
        "                            use_cache: bool = True,\n",
        "                            debug: bool = True,\n",
        "                            target_indices: Optional[np.ndarray] = None,\n",
        "                            use_class14: bool = False,\n",
        "                        ):\n",
        "    debug_str = f\"_debug{int(debug)}\"\n",
        "    train_data_type_str = f\"_{train_data_type}\"\n",
        "    class14_str = f\"_14class{int(use_class14)}\"\n",
        "    cache_path = Path(\".\") / f\"dataset_dicts_cache{train_data_type_str}{class14_str}{debug_str}.pkl\"\n",
        "    if not use_cache or not cache_path.exists():\n",
        "        print(\"Creating data...\")\n",
        "        train_meta = pd.read_csv(imgdir / \"train_meta.csv\")\n",
        "        if debug:\n",
        "            train_meta = train_meta.iloc[:1000]  # For debug....\n",
        "\n",
        "        # Load 1 image to get image size.\n",
        "        image_id = train_meta.loc[0, \"image_id\"]\n",
        "        image_path = str(imgdir / \"train\" / f\"{image_id}.png\")\n",
        "        image = cv2.imread(image_path)\n",
        "        resized_height, resized_width, ch = image.shape\n",
        "        print(f\"image shape: {image.shape}\")\n",
        "\n",
        "        dataset_dicts = []\n",
        "        for index, train_meta_row in tqdm(train_meta.iterrows(), total=len(train_meta)):\n",
        "            record = {}\n",
        "\n",
        "            image_id, height, width = train_meta_row.values\n",
        "            filename = str(imgdir / \"train\" / f\"{image_id}.png\")\n",
        "            record[\"file_name\"] = filename\n",
        "            record[\"image_id\"] = image_id\n",
        "            record[\"height\"] = resized_height\n",
        "            record[\"width\"] = resized_width\n",
        "            objs = []\n",
        "            for index2, row in train_df.query(\"image_id == @image_id\").iterrows():\n",
        "                # print(row)\n",
        "                # print(row[\"class_name\"])\n",
        "                # class_name = row[\"class_name\"]\n",
        "                class_id = row[\"class_id\"]\n",
        "                if class_id == 14:\n",
        "                    # It is \"No finding\"\n",
        "                    if use_class14:\n",
        "                        # Use this No finding class with the bbox covering all image area.\n",
        "                        bbox_resized = [0, 0, resized_width, resized_height]\n",
        "                        obj = {\n",
        "                            \"bbox\": bbox_resized,\n",
        "                            \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                            \"category_id\": class_id,\n",
        "                        }\n",
        "                        objs.append(obj)\n",
        "                    else:\n",
        "                        # This annotator does not find anything, skip.\n",
        "                        pass\n",
        "                else:\n",
        "                    # bbox_original = [int(row[\"x_min\"]), int(row[\"y_min\"]), int(row[\"x_max\"]), int(row[\"y_max\"])]\n",
        "                    h_ratio = resized_height / height\n",
        "                    w_ratio = resized_width / width\n",
        "                    bbox_resized = [\n",
        "                        float(row[\"x_min\"]) * w_ratio,\n",
        "                        float(row[\"y_min\"]) * h_ratio,\n",
        "                        float(row[\"x_max\"]) * w_ratio,\n",
        "                        float(row[\"y_max\"]) * h_ratio,\n",
        "                    ]\n",
        "                    obj = {\n",
        "                        \"bbox\": bbox_resized,\n",
        "                        \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                        \"category_id\": class_id,\n",
        "                    }\n",
        "                    objs.append(obj)\n",
        "            record[\"annotations\"] = objs\n",
        "            dataset_dicts.append(record)\n",
        "        with open(cache_path, mode=\"wb\") as f:\n",
        "            pickle.dump(dataset_dicts, f)\n",
        "\n",
        "    print(f\"Load from cache {cache_path}\")\n",
        "    with open(cache_path, mode=\"rb\") as f:\n",
        "        dataset_dicts = pickle.load(f)\n",
        "    if target_indices is not None:\n",
        "        dataset_dicts = [dataset_dicts[i] for i in target_indices]\n",
        "    return dataset_dicts\n",
        "\n",
        "\n",
        "def get_vinbigdata_dicts_test(\n",
        "    imgdir: Path, test_meta: pd.DataFrame, use_cache: bool = True, debug: bool = True,\n",
        "):\n",
        "    debug_str = f\"_debug{int(debug)}\"\n",
        "    cache_path = Path(\".\") / f\"dataset_dicts_cache_test{debug_str}.pkl\"\n",
        "    if not use_cache or not cache_path.exists():\n",
        "        print(\"Creating data...\")\n",
        "        # test_meta = pd.read_csv(imgdir / \"test_meta.csv\")\n",
        "        if debug:\n",
        "            test_meta = test_meta.iloc[:1000]  # For debug....\n",
        "\n",
        "        # Load 1 image to get image size.\n",
        "        image_id = test_meta.loc[0, \"image_id\"]\n",
        "        image_path = str(imgdir / \"test\" / f\"{image_id}.png\")\n",
        "        image = cv2.imread(image_path)\n",
        "        resized_height, resized_width, ch = image.shape\n",
        "        print(f\"image shape: {image.shape}\")\n",
        "\n",
        "        dataset_dicts = []\n",
        "        for index, test_meta_row in tqdm(test_meta.iterrows(), total=len(test_meta)):\n",
        "            record = {}\n",
        "\n",
        "            image_id, height, width = test_meta_row.values\n",
        "            filename = str(imgdir / \"test\" / f\"{image_id}.png\")\n",
        "            record[\"file_name\"] = filename\n",
        "            # record[\"image_id\"] = index\n",
        "            record[\"image_id\"] = image_id\n",
        "            record[\"height\"] = resized_height\n",
        "            record[\"width\"] = resized_width\n",
        "            # objs = []\n",
        "            # record[\"annotations\"] = objs\n",
        "            dataset_dicts.append(record)\n",
        "        with open(cache_path, mode=\"wb\") as f:\n",
        "            pickle.dump(dataset_dicts, f)\n",
        "\n",
        "    print(f\"Load from cache {cache_path}\")\n",
        "    with open(cache_path, mode=\"rb\") as f:\n",
        "        dataset_dicts = pickle.load(f)\n",
        "    return dataset_dicts"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "0mROmzBf-cjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods for prediction for this project"
      ],
      "metadata": {
        "id": "3U1CliS6-cjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Methods for prediction for this competition\n",
        "from math import ceil\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import cv2\n",
        "import detectron2\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def format_pred(labels: ndarray, boxes: ndarray, scores: ndarray) -> str:\n",
        "    pred_strings = []\n",
        "    for label, score, bbox in zip(labels, scores, boxes):\n",
        "        xmin, ymin, xmax, ymax = bbox.astype(np.int64)\n",
        "        pred_strings.append(f\"{label} {score} {xmin} {ymin} {xmax} {ymax}\")\n",
        "    return \" \".join(pred_strings)\n",
        "\n",
        "\n",
        "def predict_batch(predictor: DefaultPredictor, im_list: List[ndarray]) -> List:\n",
        "    with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
        "        inputs_list = []\n",
        "        for original_image in im_list:\n",
        "            # Apply pre-processing to image.\n",
        "            if predictor.input_format == \"RGB\":\n",
        "                # whether the model expects BGR inputs or RGB\n",
        "                original_image = original_image[:, :, ::-1]\n",
        "            height, width = original_image.shape[:2]\n",
        "            # Do not apply original augmentation, which is resize.\n",
        "            # image = predictor.aug.get_transform(original_image).apply_image(original_image)\n",
        "            image = original_image\n",
        "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
        "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
        "            inputs_list.append(inputs)\n",
        "        predictions = predictor.model(inputs_list)\n",
        "        return predictions"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Yp_3U551-cjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- utils ---\n",
        "from pathlib import Path\n",
        "from typing import Any, Union\n",
        "\n",
        "import yaml\n",
        "\n",
        "\n",
        "def save_yaml(filepath: Union[str, Path], content: Any, width: int = 120):\n",
        "    with open(filepath, \"w\") as f:\n",
        "        yaml.dump(content, f, width=width)\n",
        "\n",
        "\n",
        "def load_yaml(filepath: Union[str, Path]) -> Any:\n",
        "    with open(filepath, \"r\") as f:\n",
        "        content = yaml.full_load(f)\n",
        "    return content\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "RaDarLNK-cjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- configs ---\n",
        "thing_classes = [\n",
        "    \"Aortic enlargement\",\n",
        "    \"Atelectasis\",\n",
        "    \"Calcification\",\n",
        "    \"Cardiomegaly\",\n",
        "    \"Consolidation\",\n",
        "    \"ILD\",\n",
        "    \"Infiltration\",\n",
        "    \"Lung Opacity\",\n",
        "    \"Nodule/Mass\",\n",
        "    \"Other lesion\",\n",
        "    \"Pleural effusion\",\n",
        "    \"Pleural thickening\",\n",
        "    \"Pneumothorax\",\n",
        "    \"Pulmonary fibrosis\",\n",
        "    \"No Finding\"\n",
        "]\n",
        "category_name_to_id = {class_name: index for index, class_name in enumerate(thing_classes)}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "e-TqRayR-cja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `Flags` class is to manage experiments. I will tune these parameters through the competition to improve model's performance."
      ],
      "metadata": {
        "id": "EF5_RJgP-cja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- flags ---\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Flags:\n",
        "    # General\n",
        "    debug: bool = True\n",
        "    outdir: str = \"results/det\"\n",
        "\n",
        "    # Data config\n",
        "    imgdir_name: str = \"vinbigdata-chest-xray-abnormalities-detection\"\n",
        "    split_mode: str = \"all_train\"  # all_train or valid20\n",
        "    seed: int = 111\n",
        "    train_data_type: str = \"original\"  # original or wbf\n",
        "    use_class14: bool = False\n",
        "    # Training config\n",
        "    iter: int = 10000\n",
        "    ims_per_batch: int = 2  # images per batch, this corresponds to \"total batch size\"\n",
        "    num_workers: int = 4\n",
        "    lr_scheduler_name: str = \"WarmupMultiStepLR\"  # WarmupMultiStepLR (default) or WarmupCosineLR\n",
        "    base_lr: float = 0.000025\n",
        "    roi_batch_size_per_image: int = 720\n",
        "    eval_period: int = 10000\n",
        "    aug_kwargs: Dict = field(default_factory=lambda: {})\n",
        "\n",
        "    def update(self, param_dict: Dict) -> \"Flags\":\n",
        "        # Overwrite by `param_dict`\n",
        "        for key, value in param_dict.items():\n",
        "            if not hasattr(self, key):\n",
        "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
        "            setattr(self, key, value)\n",
        "        return self"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "PR3HxP_o-cjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"pred_scripts\"></a>\n",
        "# Prediction scripts\n",
        "\n",
        "Now the methods are ready. Main training scripts starts from here."
      ],
      "metadata": {
        "id": "wNtjpDYc-cjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputdir = Path(\"/kaggle/input\")\n",
        "traineddir = inputdir / \"vinbigdata-alb-aug-512-cos\"\n",
        "\n",
        "# flags = Flags()\n",
        "flags: Flags = Flags().update(load_yaml(str(traineddir/\"flags.yaml\")))\n",
        "print(\"flags\", flags)\n",
        "debug = flags.debug\n",
        "# flags_dict = dataclasses.asdict(flags)\n",
        "outdir = Path(flags.outdir)\n",
        "os.makedirs(str(outdir), exist_ok=True)\n",
        "\n",
        "# --- Read data ---\n",
        "datadir = inputdir / \"vinbigdata-chest-xray-abnormalities-detection\"\n",
        "if flags.imgdir_name == \"vinbigdata-chest-xray-resized-png-512x512\":\n",
        "    imgdir = inputdir/ \"vinbigdata\"\n",
        "else:\n",
        "    imgdir = inputdir / flags.imgdir_name\n",
        "\n",
        "# Read in the data CSV files\n",
        "# train = pd.read_csv(datadir / \"train.csv\")\n",
        "test_meta = pd.read_csv(inputdir / \"vinbigdata-testmeta\" / \"test_meta.csv\")\n",
        "sample_submission = pd.read_csv(datadir / \"sample_submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "fwGlSTJc-cjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "original_output_dir = cfg.OUTPUT_DIR\n",
        "cfg.OUTPUT_DIR = str(outdir)\n",
        "print(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"vinbigdata_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "# cfg.DATASETS.TEST = (\"vinbigdata_train\",)\n",
        "# cfg.TEST.EVAL_PERIOD = 50\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "# Let training initialize from model zoo\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = flags.base_lr  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = flags.iter\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = flags.roi_batch_size_per_image\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "### --- Inference & Evaluation ---\n",
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "# path to the model we just trained\n",
        "cfg.MODEL.WEIGHTS = str(traineddir/\"model_final.pth\")\n",
        "print(\"Original thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)  # 0.05\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set a custom testing threshold\n",
        "print(\"Changed  thresh\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "DatasetCatalog.register(\n",
        "    \"vinbigdata_test\", lambda: get_vinbigdata_dicts_test(imgdir, test_meta, debug=debug)\n",
        ")\n",
        "MetadataCatalog.get(\"vinbigdata_test\").set(thing_classes=thing_classes)\n",
        "metadata = MetadataCatalog.get(\"vinbigdata_test\")\n",
        "dataset_dicts = get_vinbigdata_dicts_test(imgdir, test_meta, debug=debug)\n",
        "\n",
        "if debug:\n",
        "    dataset_dicts = dataset_dicts[:100]\n",
        "\n",
        "results_list = []\n",
        "index = 0\n",
        "batch_size = 4\n",
        "\n",
        "for i in tqdm(range(ceil(len(dataset_dicts) / batch_size))):\n",
        "    inds = list(range(batch_size * i, min(batch_size * (i + 1), len(dataset_dicts))))\n",
        "    dataset_dicts_batch = [dataset_dicts[i] for i in inds]\n",
        "    im_list = [cv2.imread(d[\"file_name\"]) for d in dataset_dicts_batch]\n",
        "    outputs_list = predict_batch(predictor, im_list)\n",
        "\n",
        "    for im, outputs, d in zip(im_list, outputs_list, dataset_dicts_batch):\n",
        "        resized_height, resized_width, ch = im.shape\n",
        "        # outputs = predictor(im)\n",
        "        if index < 10:\n",
        "            # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "            v = Visualizer(\n",
        "                im[:, :, ::-1],\n",
        "                metadata=metadata,\n",
        "                scale=0.5,\n",
        "                instance_mode=ColorMode.IMAGE_BW\n",
        "                # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "            )\n",
        "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "            # cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "            cv2.imwrite(str(outdir / f\"pred_{index}.jpg\"), out.get_image()[:, :, ::-1])\n",
        "\n",
        "        image_id, dim0, dim1 = test_meta.iloc[index].values\n",
        "\n",
        "        instances = outputs[\"instances\"]\n",
        "        if len(instances) == 0:\n",
        "            # No finding, let's set 14 1 0 0 1 1x.\n",
        "            result = {\"image_id\": image_id, \"PredictionString\": \"14 1.0 0 0 1 1\"}\n",
        "        else:\n",
        "            # Find some bbox...\n",
        "            # print(f\"index={index}, find {len(instances)} bbox.\")\n",
        "            fields: Dict[str, Any] = instances.get_fields()\n",
        "            pred_classes = fields[\"pred_classes\"]  # (n_boxes,)\n",
        "            pred_scores = fields[\"scores\"]\n",
        "            # shape (n_boxes, 4). (xmin, ymin, xmax, ymax)\n",
        "            pred_boxes = fields[\"pred_boxes\"].tensor\n",
        "\n",
        "            h_ratio = dim0 / resized_height\n",
        "            w_ratio = dim1 / resized_width\n",
        "            pred_boxes[:, [0, 2]] *= w_ratio\n",
        "            pred_boxes[:, [1, 3]] *= h_ratio\n",
        "\n",
        "            pred_classes_array = pred_classes.cpu().numpy()\n",
        "            pred_boxes_array = pred_boxes.cpu().numpy()\n",
        "            pred_scores_array = pred_scores.cpu().numpy()\n",
        "\n",
        "            result = {\n",
        "                \"image_id\": image_id,\n",
        "                \"PredictionString\": format_pred(\n",
        "                    pred_classes_array, pred_boxes_array, pred_scores_array\n",
        "                ),\n",
        "            }\n",
        "        results_list.append(result)\n",
        "        index += 1"
      ],
      "metadata": {
        "trusted": true,
        "id": "lovqQEuY-cjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I set `cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0` to produce **all the detection box prediction even if confidence score is very low**.<br/>\n",
        "Actually it affects a lot to score, since competition metric is AP (Average-Precision) which is calculated using the boxes with confidence score = 0~100%."
      ],
      "metadata": {
        "id": "DRXh5z0G-cje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This submission includes only detection model's predictions\n",
        "submission_det = pd.DataFrame(results_list, columns=['image_id', 'PredictionString'])\n",
        "submission_det.to_csv(outdir/\"submission.csv\", index=False)\n",
        "submission_det"
      ],
      "metadata": {
        "trusted": true,
        "id": "OL1p2quC-cje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = submission_det.iloc[1]\n",
        "sample"
      ],
      "metadata": {
        "trusted": true,
        "id": "TJQdOnnA-cje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_image(img_path):\n",
        "    orig_image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    image /= 255.0\n",
        "    image = np.transpose(image, (2, 0, 1)).astype(np.float)\n",
        "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
        "    image = torch.unsqueeze(image, 0)\n",
        "\n",
        "    \n",
        "    plt.figure(figsize=(15,12))\n",
        "    plt.title(\"Patient Image Prediction!\", fontsize=18)\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "\n",
        "pred_image(\"./results/20210125_all_alb_aug_512_cos/pred_8.jpg\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9-Th6ulh-cjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_image(\"./results/20210125_all_alb_aug_512_cos/pred_7.jpg\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "UDibsJSW-cjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_image(\"./results/20210125_all_alb_aug_512_cos/pred_4.jpg\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LED-M6qz-cjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NlPSv-m_C1qG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}